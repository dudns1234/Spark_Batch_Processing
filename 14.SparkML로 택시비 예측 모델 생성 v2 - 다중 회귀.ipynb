{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f64da6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/20 13:03:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://0.0.0.0:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>taxi-fare-prediction</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fbc65ccf970>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('taxi-fare-prediction').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c7fe87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:===================================================>     (10 + 1) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: string (nullable = true)\n",
      " |-- tpep_dropoff_datetime: string (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "directory = \"/home/ubuntu/working/datasource\"\n",
    "trip_files = \"/trips/*\"\n",
    "\n",
    "trips_df = spark.read.csv(f\"file:///{directory}/{trip_files}\", inferSchema=True, header=True)\n",
    "trips_df.printSchema()\n",
    "\n",
    "# trip_distance: double (nullable = true) => x\n",
    "# tolls_amount: double (nullable = true) => y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1a7d86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_df.createOrReplaceTempView(\"trips\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b284238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 정제\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "    t.passenger_count,\n",
    "    PULocationID as pickup_location_id,\n",
    "    DOLocationID as dropoff_location_id,\n",
    "    t.trip_distance,\n",
    "    HOUR(tpep_pickup_datetime) as pickup_time,\n",
    "    DATE_FORMAT(TO_DATE(tpep_pickup_datetime), 'EEEE') as day_of_week,\n",
    "    \n",
    "    t.total_amount\n",
    "\n",
    "FROM trips t\n",
    "\n",
    "WHERE t.total_amount < 200\n",
    "  AND t.total_amount > 0\n",
    "  AND t.passenger_count < 5\n",
    "  AND TO_DATE(t.tpep_pickup_datetime) >= '2021-01-01'\n",
    "  AND TO_DATE(t.tpep_pickup_datetime) < '2021-08-01'\n",
    "  AND t.trip_distance < 10\n",
    "  AND t.trip_distance > 0\n",
    "\"\"\"\n",
    "\n",
    "data_df = spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58879c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- pickup_location_id: integer (nullable = true)\n",
      " |-- dropoff_location_id: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- pickup_time: integer (nullable = true)\n",
      " |-- day_of_week: string (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472dcdcb",
   "metadata": {},
   "source": [
    "Train / Test 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "082a7b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sdf, test_sdf = data_df.randomSplit([0.8,0.2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d370bdbc",
   "metadata": {},
   "source": [
    "만약에 데이터의 양이 너무 많고, 그 데이터를 오랜 시간을 들여서 전처리를 다 완료 했다고 가정.\n",
    "- 여러 모델을 만들거나 실험을 할 때에도 위의 전처리 작업을 그대로 매번 수행\n",
    "- 추후에 다시 이 데이터를 활용한다면 시간이 많이 걸릴듯....\n",
    "- 처리가 완료된 데이터를 파일이나 데이터베이스에 저장해 놓고 나중에 불러오는게 더 빠르다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17a44480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 파케이 (parquet) 형식으로 저장\n",
    "save_dir = \"/home/ubuntu/working/spark-examples/data/ml-data\"\n",
    "\n",
    "# Spark DataFrame의 write 메소드를 이용해 데이터를 파일 또는 데이터베이스에 저장할 수 있다.\n",
    "train_sdf.write.format(\"parquet\").save(f\"{save_dir}/train/\")\n",
    "test_sdf.write.format(\"parquet\").save(f\"{save_dir}/test/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0431b37",
   "metadata": {},
   "source": [
    "카테고리 -> 원핫인코딩\n",
    "실수 -> standardscaling\n",
    "원핫 + standard => Assemble해줌 -> feature\n",
    "이 과정이 pipeline임."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c64103b",
   "metadata": {},
   "source": [
    "# 파이프라인 정의\n",
    "파이프라인 정의를 위한 stage 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff8db315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이프라인 정의에 넣을 과정(stage)을 모아 놓을 리스트\n",
    "stages = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2b9bcc",
   "metadata": {},
   "source": [
    "## OneHotEncoding Stage\n",
    "- `pickup_location_id`\n",
    "- `dropoff_location_id`\n",
    "- `day_of_week`\n",
    "\n",
    "`pickup_location_id`, `dropoff_location_id`는 숫자 형식의 데이터\n",
    "- 숫자 형식의 데이터는 `OneHotEncoding`이 불가능\n",
    "- `StringIndexer` Transformer를 활용해 숫자형 데이터를 문자열로 취급하게끔 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d99e714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_5e82c1de92a4,\n",
       " OneHotEncoder_79630feee761,\n",
       " StringIndexer_373a484bce43,\n",
       " OneHotEncoder_ef46306b6083,\n",
       " StringIndexer_8a31a14b3d37,\n",
       " OneHotEncoder_4ad983ec6e38]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pick, drop -> int를 문자열로 변환해서 카테고리 형식이 되도록 \n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "# OneHotEncoding을 수행할 컬럼\n",
    "cat_features = [\n",
    "    \"pickup_location_id\",\n",
    "    \"dropoff_location_id\",\n",
    "    \"day_of_week\" # day는 이미 str이여서 굳이 안해줘도 되지만 걍 같이 해주겠음\n",
    "]\n",
    "\n",
    "for c in cat_features:\n",
    "    # 1. 데이터를 문자열 형식으로 바꿔준다.\n",
    "    cat_indexer = StringIndexer(inputCol=c, outputCol=c+\"_idx\").setHandleInvalid(\"keep\") # 위에서 이미 null값 체크 및 제거, 중복값 처리, EDA를 충분히 하고 진행하는 것임\n",
    "    \n",
    "    # 2. OneHotEncoding 수행\n",
    "    onehot_encoder = OneHotEncoder(\n",
    "        inputCols = [cat_indexer.getOutputCol()],\n",
    "        outputCols = [c+\"_onehot\"] # 얘만 assembling 해주기\n",
    "    )\n",
    "    \n",
    "    stages += [cat_indexer, onehot_encoder]\n",
    "    \n",
    "stages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a10cc94",
   "metadata": {},
   "source": [
    "## Standard Scaling Stage\n",
    "- 숫자형 데이터들에 대한 표준화 수행\n",
    "- `passenger_count`, `trip_distance`, `pickup_time`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5a66734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_5e82c1de92a4,\n",
       " OneHotEncoder_79630feee761,\n",
       " StringIndexer_373a484bce43,\n",
       " OneHotEncoder_ef46306b6083,\n",
       " StringIndexer_8a31a14b3d37,\n",
       " OneHotEncoder_4ad983ec6e38,\n",
       " VectorAssembler_bc1f54be5a52,\n",
       " StandardScaler_a87910c3d334,\n",
       " VectorAssembler_860825e843da,\n",
       " StandardScaler_a71d1ca121b5,\n",
       " VectorAssembler_73b19994c644,\n",
       " StandardScaler_ab9ca325122d]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 컬럼의 데이터를 벡터화 시키고, Standard Scaling을 수행\n",
    "## 반드시 이차원 배열로 되어 있어야함.\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "num_features = [\n",
    "    \"passenger_count\",\n",
    "    \"trip_distance\",\n",
    "    \"pickup_time\"\n",
    "]\n",
    "\n",
    "for n in num_features:\n",
    "    # 1. 벡터화 Stage\n",
    "    num_assembler = VectorAssembler(inputCols=[n], outputCol=n+\"_vector\")\n",
    "    \n",
    "    # 2. StandardScaler Stage\n",
    "    num_scaler = StandardScaler(inputCol=num_assembler.getOutputCol(), outputCol=n+\"_scaled\") # 얘만 assembling 해주기\n",
    "    \n",
    "    stages += [num_assembler, num_scaler]\n",
    "    \n",
    "stages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93873af2",
   "metadata": {},
   "source": [
    "## Feature Assemble Stage\n",
    "- 컬럼 명 뒤에 `_onehot`이 붙거나 `_scaled`가 붙은 컬럼만 Feature Vector로 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b255c74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pickup_location_id_onehot',\n",
       " 'dropoff_location_id_onehot',\n",
       " 'day_of_week_onehot',\n",
       " 'passenger_count_scaled',\n",
       " 'trip_distance_scaled',\n",
       " 'pickup_time_scaled']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assembler_inputs = [ c + \"_onehot\" for c in cat_features] + [ n + \"_scaled\" for n in num_features]\n",
    "assembler_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58e2a9df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_5e82c1de92a4,\n",
       " OneHotEncoder_79630feee761,\n",
       " StringIndexer_373a484bce43,\n",
       " OneHotEncoder_ef46306b6083,\n",
       " StringIndexer_8a31a14b3d37,\n",
       " OneHotEncoder_4ad983ec6e38,\n",
       " VectorAssembler_bc1f54be5a52,\n",
       " StandardScaler_a87910c3d334,\n",
       " VectorAssembler_860825e843da,\n",
       " StandardScaler_a71d1ca121b5,\n",
       " VectorAssembler_73b19994c644,\n",
       " StandardScaler_ab9ca325122d,\n",
       " VectorAssembler_a0c530680826]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n",
    "\n",
    "stages.append(feature_assembler)\n",
    "stages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5034b076",
   "metadata": {},
   "source": [
    "## Pipeline 구성\n",
    "순서대로 구성된 Stage를 한꺼번에 수행할 파이프라인 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9baff713",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "transform_stages = stages\n",
    "pipeline = Pipeline(stages=transform_stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59ec4d0",
   "metadata": {},
   "source": [
    "## 데이터를 파이프라인에 통과시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7cc72d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# transformer의 fit : 변환을 하기 위한 수 또는 방법을 구하는 과정\n",
    "fitted_transformer = pipeline.fit(train_sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1c69276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- pickup_location_id: integer (nullable = true)\n",
      " |-- dropoff_location_id: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- pickup_time: integer (nullable = true)\n",
      " |-- day_of_week: string (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- pickup_location_id_idx: double (nullable = false)\n",
      " |-- pickup_location_id_onehot: vector (nullable = true)\n",
      " |-- dropoff_location_id_idx: double (nullable = false)\n",
      " |-- dropoff_location_id_onehot: vector (nullable = true)\n",
      " |-- day_of_week_idx: double (nullable = false)\n",
      " |-- day_of_week_onehot: vector (nullable = true)\n",
      " |-- passenger_count_vector: vector (nullable = true)\n",
      " |-- passenger_count_scaled: vector (nullable = true)\n",
      " |-- trip_distance_vector: vector (nullable = true)\n",
      " |-- trip_distance_scaled: vector (nullable = true)\n",
      " |-- pickup_time_vector: vector (nullable = true)\n",
      " |-- pickup_time_scaled: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# transform : 데이터를 변환\n",
    "vec_train_sdf = fitted_transformer.transform(train_sdf)\n",
    "vec_train_sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72d30d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[Stage 22:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+\n",
      "|            features|total_amount|\n",
      "+--------------------+------------+\n",
      "|(532,[62,311,526,...|         6.3|\n",
      "|(532,[62,280,525,...|         8.8|\n",
      "|(532,[62,280,527,...|         9.8|\n",
      "|(532,[62,279,525,...|       10.55|\n",
      "|(532,[62,298,522,...|        10.8|\n",
      "+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "vec_train_sdf.select(\"features\", \"total_amount\").show(5) # (피처의 개수, [원핫인코딩된 번호"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a48349",
   "metadata": {},
   "source": [
    "## 모델 생성 및 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "11712457",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(\n",
    "    maxIter = 50,\n",
    "    solver = 'normal', # 최적화 방식\n",
    "    labelCol = 'total_amount',\n",
    "    featuresCol = 'features'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2acb1e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/20 14:25:15 WARN Instrumentation: [2ae0ccec] regParam is zero, which might cause numerical instability and overfitting.\n",
      "23/11/20 14:25:45 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/11/20 14:25:45 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "23/11/20 14:27:36 WARN InstanceBuilder$NativeLAPACK: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "23/11/20 14:27:36 WARN Instrumentation: [2ae0ccec] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "23/11/20 14:27:37 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "23/11/20 14:27:37 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lr_model = lr.fit(vec_train_sdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b907069",
   "metadata": {},
   "source": [
    "## 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "72876292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이프라인을 이용해 test_sdf 변환\n",
    "vec_test_sdf = fitted_transformer.transform(test_sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e7a28511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vec_test_sdf로 예측\n",
    "predictions = lr_model.transform(vec_test_sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4e7b2774",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[Stage 27:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+-------------------+-------------+-----------+-----------+------------+----------------------+-------------------------+-----------------------+--------------------------+---------------+------------------+----------------------+----------------------+--------------------+--------------------+------------------+--------------------+--------------------+------------------+\n",
      "|passenger_count|pickup_location_id|dropoff_location_id|trip_distance|pickup_time|day_of_week|total_amount|pickup_location_id_idx|pickup_location_id_onehot|dropoff_location_id_idx|dropoff_location_id_onehot|day_of_week_idx|day_of_week_onehot|passenger_count_vector|passenger_count_scaled|trip_distance_vector|trip_distance_scaled|pickup_time_vector|  pickup_time_scaled|            features|        prediction|\n",
      "+---------------+------------------+-------------------+-------------+-----------+-----------+------------+----------------------+-------------------------+-----------------------+--------------------------+---------------+------------------+----------------------+----------------------+--------------------+--------------------+------------------+--------------------+--------------------+------------------+\n",
      "|              0|                 4|                 79|          0.7|         23|   Saturday|       12.35|                  62.0|         (262,[62],[1.0])|                   18.0|          (260,[18],[1.0])|            4.0|     (7,[4],[1.0])|                 [0.0]|                 [0.0]|               [0.7]|[0.3960622427197008]|            [23.0]|  [4.53624377431206]|(532,[62,280,526,...|10.520957566446452|\n",
      "|              0|                 4|                137|          1.5|         17|     Friday|        11.8|                  62.0|         (262,[62],[1.0])|                   25.0|          (260,[25],[1.0])|            0.0|     (7,[0],[1.0])|                 [0.0]|                 [0.0]|               [1.5]|[0.8487048058279304]|            [17.0]|[3.3528758331871744]|(532,[62,287,522,...| 13.65652098094591|\n",
      "|              0|                 4|                148|          1.0|         19|     Friday|        12.3|                  62.0|         (262,[62],[1.0])|                   39.0|          (260,[39],[1.0])|            0.0|     (7,[0],[1.0])|                 [0.0]|                 [0.0]|               [1.0]|[0.5658032038852869]|            [19.0]|[3.7473318135621363]|(532,[62,301,522,...| 12.76767802677514|\n",
      "|              0|                 4|                234|          1.7|         18|   Saturday|        13.3|                  62.0|         (262,[62],[1.0])|                   11.0|          (260,[11],[1.0])|            4.0|     (7,[4],[1.0])|                 [0.0]|                 [0.0]|               [1.7]|[0.9618654466049877]|            [18.0]|[3.5501038233746556]|(532,[62,273,526,...|  13.8888089892157|\n",
      "|              0|                 7|                  7|          1.4|         14|     Friday|         8.3|                  63.0|         (262,[63],[1.0])|                   55.0|          (260,[55],[1.0])|            0.0|     (7,[0],[1.0])|                 [0.0]|                 [0.0]|               [1.4]|[0.7921244854394016]|            [14.0]| [2.761191862624732]|(532,[63,317,522,...|  9.67076737912389|\n",
      "|              0|                 7|                100|          4.6|         10|     Monday|        26.8|                  63.0|         (262,[63],[1.0])|                   30.0|          (260,[30],[1.0])|            5.0|     (7,[5],[1.0])|                 [0.0]|                 [0.0]|               [4.6]|[2.6026947378723198]|            [10.0]|[1.9722799018748085]|(532,[63,292,527,...|22.927184691166815|\n",
      "|              0|                 7|                179|          0.7|         11|  Wednesday|         5.3|                  63.0|         (262,[63],[1.0])|                   75.0|          (260,[75],[1.0])|            2.0|     (7,[2],[1.0])|                 [0.0]|                 [0.0]|               [0.7]|[0.3960622427197008]|            [11.0]|[2.1695078920622892]|(532,[63,337,524,...| 6.758567784586714|\n",
      "|              0|                 7|                223|          1.7|          9|    Tuesday|        11.8|                  63.0|         (262,[63],[1.0])|                   69.0|          (260,[69],[1.0])|            3.0|     (7,[3],[1.0])|                 [0.0]|                 [0.0]|               [1.7]|[0.9618654466049877]|             [9.0]|[1.7750519116873278]|(532,[63,331,525,...|10.683491846025056|\n",
      "|              0|                13|                 13|          0.1|          7|  Wednesday|        55.3|                  47.0|         (262,[47],[1.0])|                   44.0|          (260,[44],[1.0])|            2.0|     (7,[2],[1.0])|                 [0.0]|                 [0.0]|               [0.1]|[0.0565803203885287]|             [7.0]| [1.380595931312366]|(532,[47,306,524,...|6.8158905129782985|\n",
      "|              0|                13|                 13|          0.7|         16|  Wednesday|        10.3|                  47.0|         (262,[47],[1.0])|                   44.0|          (260,[44],[1.0])|            2.0|     (7,[2],[1.0])|                 [0.0]|                 [0.0]|               [0.7]|[0.3960622427197008]|            [16.0]|[3.1556478429996937]|(532,[47,306,524,...|  9.48794145133163|\n",
      "|              0|                13|                 24|          7.4|          9|     Friday|        37.4|                  47.0|         (262,[47],[1.0])|                   48.0|          (260,[48],[1.0])|            0.0|     (7,[0],[1.0])|                 [0.0]|                 [0.0]|               [7.4]| [4.186943708751124]|             [9.0]|[1.7750519116873278]|(532,[47,310,522,...| 33.81738044712699|\n",
      "|              0|                13|                 33|          3.9|         18|    Tuesday|       21.95|                  47.0|         (262,[47],[1.0])|                   62.0|          (260,[62],[1.0])|            3.0|     (7,[3],[1.0])|                 [0.0]|                 [0.0]|               [3.9]| [2.206632495152619]|            [18.0]|[3.5501038233746556]|(532,[47,324,525,...|21.187066662973017|\n",
      "|              0|                13|                 34|          4.1|         13|     Monday|       22.55|                  47.0|         (262,[47],[1.0])|                  144.0|         (260,[144],[1.0])|            5.0|     (7,[5],[1.0])|                 [0.0]|                 [0.0]|               [4.1]| [2.319793135929676]|            [13.0]| [2.563963872437251]|(532,[47,406,527,...|21.755690286476216|\n",
      "|              0|                13|                 48|          4.6|         22|   Saturday|        24.8|                  47.0|         (262,[47],[1.0])|                   12.0|          (260,[12],[1.0])|            4.0|     (7,[4],[1.0])|                 [0.0]|                 [0.0]|               [4.6]|[2.6026947378723198]|            [22.0]|[4.3390157841245784]|(532,[47,274,526,...|24.197658432375192|\n",
      "|              0|                13|                 79|          4.5|         10|     Friday|       26.35|                  47.0|         (262,[47],[1.0])|                   18.0|          (260,[18],[1.0])|            0.0|     (7,[0],[1.0])|                 [0.0]|                 [0.0]|               [4.5]|[2.5461144174837913]|            [10.0]|[1.9722799018748085]|(532,[47,280,522,...| 24.08209010841602|\n",
      "|              0|                13|                 88|          1.3|         20|  Wednesday|       14.15|                  47.0|         (262,[47],[1.0])|                   56.0|          (260,[56],[1.0])|            2.0|     (7,[2],[1.0])|                 [0.0]|                 [0.0]|               [1.3]|[0.7355441650508731]|            [20.0]| [3.944559803749617]|(532,[47,318,524,...|10.615869679790933|\n",
      "|              0|                13|                100|          4.4|          6|   Thursday|       21.62|                  47.0|         (262,[47],[1.0])|                   30.0|          (260,[30],[1.0])|            1.0|     (7,[1],[1.0])|                 [0.0]|                 [0.0]|               [4.4]| [2.489534097095263]|             [6.0]|[1.1833679411248852]|(532,[47,292,523,...| 23.84349115933483|\n",
      "|              0|                13|                145|          7.9|         15|   Saturday|       41.27|                  47.0|         (262,[47],[1.0])|                   58.0|          (260,[58],[1.0])|            4.0|     (7,[4],[1.0])|                 [0.0]|                 [0.0]|               [7.9]| [4.469845310693767]|            [15.0]| [2.958419852812213]|(532,[47,320,526,...| 36.22208141766147|\n",
      "|              0|                13|                162|          6.8|          7|    Tuesday|        31.8|                  47.0|         (262,[47],[1.0])|                    8.0|           (260,[8],[1.0])|            3.0|     (7,[3],[1.0])|                 [0.0]|                 [0.0]|               [6.8]| [3.847461786419951]|             [7.0]| [1.380595931312366]|(532,[47,270,525,...| 32.25152665870132|\n",
      "|              0|                13|                164|          5.8|          8|     Monday|       28.55|                  47.0|         (262,[47],[1.0])|                   20.0|          (260,[20],[1.0])|            5.0|     (7,[5],[1.0])|                 [0.0]|                 [0.0]|               [5.8]|[3.2816585825346642]|             [8.0]|[1.5778239214998468]|(532,[47,282,527,...|28.344297416587956|\n",
      "+---------------+------------------+-------------------+-------------+-----------+-----------+------------+----------------------+-------------------------+-----------------------+--------------------------+---------------+------------------+----------------------+----------------------+--------------------+--------------------+------------------+--------------------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "71cc98ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[passenger_count: int, pickup_location_id: int, dropoff_location_id: int, trip_distance: double, pickup_time: int, day_of_week: string, total_amount: double, pickup_location_id_idx: double, pickup_location_id_onehot: vector, dropoff_location_id_idx: double, dropoff_location_id_onehot: vector, day_of_week_idx: double, day_of_week_onehot: vector, passenger_count_vector: vector, passenger_count_scaled: vector, trip_distance_vector: vector, trip_distance_scaled: vector, pickup_time_vector: vector, pickup_time_scaled: vector, features: vector, prediction: double]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예측한 결과를 따로 확인 할 때는 일반적으로 조회만 일어나기 때문에 캐시 처리를 해 주는 것이 좋다.\n",
    "predictions.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1fa9f0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[Stage 28:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+------------+------------------+\n",
      "|trip_distance|day_of_week|total_amount|        prediction|\n",
      "+-------------+-----------+------------+------------------+\n",
      "|          0.7|   Saturday|       12.35|10.520957566446452|\n",
      "|          1.5|     Friday|        11.8| 13.65652098094591|\n",
      "|          1.0|     Friday|        12.3| 12.76767802677514|\n",
      "|          1.7|   Saturday|        13.3|  13.8888089892157|\n",
      "|          1.4|     Friday|         8.3|  9.67076737912389|\n",
      "|          4.6|     Monday|        26.8|22.927184691166815|\n",
      "|          0.7|  Wednesday|         5.3| 6.758567784586714|\n",
      "|          1.7|    Tuesday|        11.8|10.683491846025056|\n",
      "|          0.1|  Wednesday|        55.3|6.8158905129782985|\n",
      "|          0.7|  Wednesday|        10.3|  9.48794145133163|\n",
      "|          7.4|     Friday|        37.4| 33.81738044712699|\n",
      "|          3.9|    Tuesday|       21.95|21.187066662973017|\n",
      "|          4.1|     Monday|       22.55|21.755690286476216|\n",
      "|          4.6|   Saturday|        24.8|24.197658432375192|\n",
      "|          4.5|     Friday|       26.35| 24.08209010841602|\n",
      "|          1.3|  Wednesday|       14.15|10.615869679790933|\n",
      "|          4.4|   Thursday|       21.62| 23.84349115933483|\n",
      "|          7.9|   Saturday|       41.27| 36.22208141766147|\n",
      "|          6.8|    Tuesday|        31.8| 32.25152665870132|\n",
      "|          5.8|     Monday|       28.55|28.344297416587956|\n",
      "+-------------+-----------+------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions.select(\"trip_distance\",\"day_of_week\",\"total_amount\",\"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e9db9f3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.2685055544721795"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model.summary.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4bb85eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.794446430313654"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model.summary.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "72a9cba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
